{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "from sklearn.model_selection import train_test_split\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Tree Leaner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DTlearner(object):\n",
    "   \n",
    "#   class for tree building\n",
    "    class Node:\n",
    "        def __init__(self,data=None):\n",
    "            self.data=data\n",
    "            self.left=None\n",
    "            self.right=None\n",
    "        \n",
    "        \n",
    "    \n",
    "    def __init__(self, criterion='squared_error',size=1):\n",
    "        self._root=None\n",
    "        \n",
    "        # Error criteria\n",
    "        self._criterion = criterion\n",
    "        \n",
    "        # Batch size jispr splitting hona bnd hojae\n",
    "        self._SIZE=size\n",
    "        \n",
    "        self.count=0\n",
    "        \n",
    "        \n",
    "    def add_evidence(self,X,y):\n",
    "        self._decision_tree(X,y)\n",
    "    \n",
    "    def _decision_tree(self,X,y):\n",
    "        \n",
    "        self._root = self._build_tree(X,y,self._root)\n",
    "        \n",
    "    def _build_tree(self,X,y,node):\n",
    "        # Selecting best feature and its value\n",
    "        feature, value = self._get_best_feature(X,y)\n",
    "        \n",
    "        \n",
    "        # Making a node of that features to make this feature as a Root Node\n",
    "        node = self.Node([feature,value])\n",
    "        \n",
    "        # Concat the dataframe \n",
    "        concat_df = pd.concat([X,y],axis=1)\n",
    "        \n",
    "        # creating dataframes for left and right split on the basis of best feature and its values\n",
    "        # split less than best value goes on left side and greater one goes on right side\n",
    "        left_split = concat_df[concat_df[feature]<value]\n",
    "        right_split = concat_df[concat_df[feature]>value]\n",
    "        \n",
    "        \n",
    "        # When both data splits are greater than our batch size \n",
    "        if (left_split.shape[0]>self._SIZE) & (right_split.shape[0]>self._SIZE):\n",
    "            # send the left split on the left side of the node\n",
    "            \n",
    "            node.left = self._build_tree(left_split.iloc[:,:-1],left_split.iloc[:,-1:],node.left)\n",
    "            \n",
    "            # send the right split on the right side of the node\n",
    "            node.right = self._build_tree(right_split.iloc[:,:-1],right_split.iloc[:,-1:],node.right)\n",
    "            \n",
    "        # When both are less than batch size\n",
    "        elif (left_split.shape[0]<=self._SIZE) & (right_split.shape[0]<=self._SIZE):\n",
    "\n",
    "            # There are 3 possibilities\n",
    "            \n",
    "#             1) when both are != 0 then simply take the mean of Y attribute and make it as leaf_node\n",
    "            if (left_split.shape[0]!=0) & (right_split.shape[0]!=0):\n",
    "\n",
    "                node.left = self.Node(['leaf_node',np.mean(left_split.iloc[:,-1:])])\n",
    "                node.right = self.Node(['leaf_node',np.mean(right_split.iloc[:,-1:])])\n",
    "           \n",
    "        \n",
    "#           2) when left_split is 0 and right_split>0 then make only a node on right side \n",
    "            elif (left_split.shape[0]==0):\n",
    "              \n",
    "                node.right = self.Node(['leaf_node',np.mean(right_split.iloc[:,-1:])])\n",
    "\n",
    "#           3) when right_split is 0 and left>0 then make only a node on left side \n",
    "                \n",
    "            elif (right_split.shape[0]==0):\n",
    "                node.left = self.Node(['leaf_node',np.mean(left_split.iloc[:,-1:])])\n",
    "        \n",
    "        \n",
    "#         When only left is small but not the right one\n",
    "        elif (left_split.shape[0]<=self._SIZE):\n",
    "            \n",
    "#             Make a leaf_node on left side and expand the tree on the right side\n",
    "            if (left_split.shape[0]!=0):\n",
    "                node.left = self.Node(['leaf_node',np.mean(left_split.iloc[:,-1:])])\n",
    "            \n",
    "            node.right = self._build_tree(right_split.iloc[:,:-1],right_split.iloc[:,-1:],node.right)\n",
    "\n",
    "#       When only right is small but not the left\n",
    "        elif (right_split.shape[0]<=self._SIZE):\n",
    "            \n",
    "            if (right_split.shape[0]!=0):\n",
    "                node.right = self.Node(['leaf_node',np.mean(right_split.iloc[:,-1:])])\n",
    "                \n",
    "            node.left = self._build_tree(left_split.iloc[:,:-1],left_split.iloc[:,-1:],node.left)\n",
    "\n",
    "\n",
    "\n",
    "        return node\n",
    "        \n",
    "    def _get_best_feature(self,X,y):\n",
    "        points= {}\n",
    "      \n",
    "\n",
    "\n",
    "    #Selecting all features\n",
    "\n",
    "        for feature in X.columns:\n",
    "            i=0\n",
    "\n",
    "#             Sort the values in ascending order\n",
    "            _arr= X[feature].sort_values(ascending=True).values\n",
    "            errors = []\n",
    "            averages = []\n",
    "            \n",
    "#             Calculate best value for the selected feature having min error value\n",
    "            while i<X.shape[0]-1:\n",
    "                avg = sum(_arr[i:i+2])/2\n",
    "            \n",
    "                left_mean = np.mean(_arr[_arr<avg])\n",
    "                right_mean = np.mean(_arr[_arr>avg])\n",
    "\n",
    "\n",
    "                if self._criterion == 'squared_error':\n",
    "                    error = sum((np.array(y)-left_mean)**2) + sum((np.array(y)-right_mean)**2)\n",
    "             \n",
    "                elif self._criterion == 'mse':\n",
    "                    error = (sum((np.array(y)-left_mean)**2) + sum((np.array(y)-right_mean)**2))/len(y)\n",
    "                \n",
    "                elif self._criterion == 'mae':\n",
    "                    error = (sum(abs(np.array(y)-left_mean)) + sum(abs(np.array(y)-right_mean)))/len(y)\n",
    "\n",
    "                errors.append(error)\n",
    "                averages.append(avg)\n",
    "                \n",
    "                i=i+1\n",
    "                \n",
    "#             Dictionay me ek feature ke tmam errors me se sbse min error wali avg value jae gi us feature ke against\n",
    "            points[feature] = [averages[np.argmin(errors)],min(errors)]\n",
    "             \n",
    "        \n",
    "        feature = ''\n",
    "        ssr = np.inf\n",
    "        point = 0\n",
    "#       upr ka process hr ek feature pr chalega tmam features ke against unki avg value or min errors agai hain\n",
    "#       ab hm yhan woh ek feature dekhain gai jiska error sbse km hoga and then uski value ko return kren gai\n",
    "        for key, value in points.items():\n",
    "            if ssr >= value[1]:\n",
    "                feature = key\n",
    "                ssr = value[1]\n",
    "                point = value[0]\n",
    "\n",
    "        \n",
    "        return (feature,point)\n",
    "    \n",
    "    # It take each row and start predicting values by passing each row in _predict function\n",
    "    # ravel : will convert the predicted array into 1D array\n",
    "    def query(self,X_test):\n",
    "        results = []\n",
    "        \n",
    "        for index,row in X_test.iterrows():\n",
    "            results.append(self._predict(row,self._root))\n",
    "            \n",
    "        return np.ravel(results)\n",
    "    \n",
    "    \n",
    "    def _predict(self,row,node):\n",
    "        \"\"\"Params: \n",
    "            row: A entry of data\n",
    "            node: the node of the tree\"\"\"\n",
    "        \n",
    "        if node is None:\n",
    "            return 'None return'\n",
    "        \n",
    "        # If Node type is leaf_node then return the value part of that node\n",
    "        elif node.data[0]=='leaf_node':\n",
    "            return node.data[1].values\n",
    "        \n",
    "        # Select the feature value of that feature which is specified in node feature part\n",
    "        value = row[node.data[0]]\n",
    "        # If node value is greater then move the tree to the left side\n",
    "        if node.data[1]>value:\n",
    "            # r stores the predicted value\n",
    "            r = self._predict(row,node.left)\n",
    "            \n",
    "            # Here None return means that at the left side there is no node hence we have to move the tree to the right so that we can get the prediction\n",
    "            if r=='None return':\n",
    "                return self._predict(row,node.right)\n",
    "            return r\n",
    "        else:\n",
    "            # If node value is less then move the tree to the left side\n",
    "            \n",
    "            r = self._predict(row,node.right)\n",
    "            \n",
    "            # Here None return means that at the right side there is no node hence we have to move the tree to the left so that we can get the prediction\n",
    "            if r=='None return':\n",
    "                return self._predict(row,node.left)\n",
    "            \n",
    "            return r\n",
    "        \n",
    "    \n",
    "    # This function will print the whole tree in Left-ROOT-Right format\n",
    "    def inorder(self,node):\n",
    "        \n",
    "        if node is None:\n",
    "            \n",
    "            return\n",
    "        \n",
    "        self.inorder(node.left)\n",
    "        print(node.data[0],node.data[1])\n",
    "        self.inorder(node.right)\n",
    "    \n",
    "    # This function will give us the main root of the tree\n",
    "    def getRoot(self):\n",
    "        return self._root\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regression Tree Leaner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RTlearner(object):\n",
    "   \n",
    "#   class for tree building\n",
    "    class Node:\n",
    "        def __init__(self,data=None):\n",
    "            self.data=data\n",
    "            self.left=None\n",
    "            self.right=None\n",
    "        \n",
    "        \n",
    "    \n",
    "    def __init__(self,size=20):\n",
    "        self._root=None\n",
    "        self._SIZE=size\n",
    "        \n",
    "    def add_evidence(self,X,y):\n",
    "        self._decision_tree(X,y)\n",
    "    \n",
    "    def _decision_tree(self,X,y):\n",
    "        \n",
    "        self._root = self._build_tree(X,y,self._root)\n",
    "    \n",
    "    # this function builds the tree same as i mention in above learner\n",
    "    def _build_tree(self,X,y,node):\n",
    "        feature, value = self._get_best_feature(X)\n",
    "        \n",
    "        \n",
    "        node = self.Node([feature,value])\n",
    "        \n",
    "        concat_df = pd.concat([X,y],axis=1)\n",
    "        \n",
    "        left_split = concat_df[concat_df[feature]<value]\n",
    "        right_split = concat_df[concat_df[feature]>value]\n",
    "        \n",
    "        \n",
    "        if (left_split.shape[0]>self._SIZE) & (right_split.shape[0]>self._SIZE):\n",
    "            \n",
    "            node.left = self._build_tree(left_split.iloc[:,:-1],left_split.iloc[:,-1:],node.left)\n",
    "            \n",
    "            node.right = self._build_tree(right_split.iloc[:,:-1],right_split.iloc[:,-1:],node.right)\n",
    "            \n",
    "        elif (left_split.shape[0]<=self._SIZE) & (right_split.shape[0]<=self._SIZE):\n",
    "\n",
    "            if (left_split.shape[0]!=0) & (right_split.shape[0]!=0):\n",
    "                \n",
    "                node.left = self.Node(['leaf_node',np.mean(left_split.iloc[:,-1:])])\n",
    "                node.right = self.Node(['leaf_node',np.mean(right_split.iloc[:,-1:])])\n",
    "           \n",
    "            \n",
    "            elif (left_split.shape[0]==0) & (right_split.shape[0]==0):\n",
    "                    return\n",
    "            elif (left_split.shape[0]==0):\n",
    "                \n",
    "                node.right = self.Node(['leaf_node',np.mean(right_split.iloc[:,-1:])])\n",
    "\n",
    "                \n",
    "            elif (right_split.shape[0]==0):\n",
    "                \n",
    "                node.left = self.Node(['leaf_node',np.mean(left_split.iloc[:,-1:])])\n",
    "        \n",
    "        \n",
    "        elif (left_split.shape[0]<=self._SIZE):\n",
    "            \n",
    "            if (left_split.shape[0]!=0):\n",
    "                node.left = self.Node(['leaf_node',np.mean(left_split.iloc[:,-1:])])\n",
    "            \n",
    "            node.right = self._build_tree(right_split.iloc[:,:-1],right_split.iloc[:,-1:],node.right)\n",
    "\n",
    "        elif (right_split.shape[0]<=self._SIZE):\n",
    "            \n",
    "            if (right_split.shape[0]!=0):\n",
    "                node.right = self.Node(['leaf_node',np.mean(right_split.iloc[:,-1:])])\n",
    "                \n",
    "            node.left = self._build_tree(left_split.iloc[:,:-1],left_split.iloc[:,-1:],node.left)\n",
    "\n",
    "\n",
    "\n",
    "        return node\n",
    "        \n",
    "    # here a random feature is selected for the tree building\n",
    "    # we select 2 random values of that feature and calulate the mean and return them\n",
    "    def _get_best_feature(self,X):\n",
    "        f_ind=np.random.randint(X.shape[1])\n",
    "        feature = X.columns[f_ind]\n",
    "\n",
    "        i = np.random.randint(X.shape[0])\n",
    "        j = np.random.randint(X.shape[0])\n",
    "\n",
    "        value = (X[feature].iloc[i] + X[feature].iloc[j])/2\n",
    "        return (feature,value)\n",
    "\n",
    "        \n",
    "    \n",
    "    def query(self,X_test):\n",
    "        results = []\n",
    "        \n",
    "        for index,row in X_test.iterrows():\n",
    "            results.append(self._predict(row,self._root))\n",
    "            \n",
    "        return np.ravel(results)\n",
    "    \n",
    "    def _predict(self,row,node):\n",
    "        \n",
    "        if node is None:\n",
    "            return 'None return'\n",
    "        \n",
    "        elif node.data[0]=='leaf_node':\n",
    "            return node.data[1].values\n",
    "        \n",
    "        \n",
    "        value = row[node.data[0]]\n",
    "        if node.data[1]>value:\n",
    "            r = self._predict(row,node.left)\n",
    "            \n",
    "            if r=='None return':\n",
    "                return self._predict(row,node.right)\n",
    "            return r\n",
    "        else:\n",
    "            \n",
    "            r = self._predict(row,node.right)\n",
    "            \n",
    "            if r=='None return':\n",
    "                return self._predict(row,node.left)\n",
    "            \n",
    "            return r\n",
    "        \n",
    "    \n",
    "    def inorder(self,node):\n",
    "        \n",
    "        if node is None:\n",
    "            \n",
    "            return\n",
    "        \n",
    "        self.inorder(node.left)\n",
    "        print(node.data[0],node.data[1])\n",
    "        self.inorder(node.right)\n",
    "    \n",
    "    def getRoot(self):\n",
    "        return self._root\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bag Learner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BagLearner:\n",
    "    \n",
    "    \"\"\"BagLearner train multiple learner or bags of leraner to predict the values\"\"\"\n",
    "    def __init__(self,learner,kwargs={'size':1}, bags=10, boost= False, verbose=False):\n",
    "        self.kwargs = kwargs\n",
    "        self.learner = learner\n",
    "        self.boost = boost\n",
    "        self.verbose = verbose\n",
    "        self.bags = bags\n",
    "        self.learners = []\n",
    "        \n",
    "        if verbose:\n",
    "            print('BagLeaner()')\n",
    "        \n",
    "        \n",
    "        \n",
    "    def add_evidence(self,X_train,y_train):\n",
    "        \n",
    "        # if boost is true then we use bagging technique and then train the individual model\n",
    "        if self.boost:\n",
    "            df = pd.concat([X_train,y_train],axis=1)\n",
    "           \n",
    "            for i in range(0,self.bags):\n",
    "                df2 = df.sample(df.shape[0],replace=True)\n",
    "            \n",
    "                self.learners.append(self.learner(**self.kwargs))\n",
    "                self.learners[i].add_evidence(X_train,y_train)\n",
    "        else:\n",
    "            \n",
    "             for i in range(0,self.bags):\n",
    "                self.learners.append(self.learner(**self.kwargs))\n",
    "                self.learners[i].add_evidence(X_train,y_train)\n",
    "    \n",
    "    # Here every gives us multiple prediction for a particular entry we take the mean of all prediction\n",
    "    def query(self,X_test):\n",
    "        y_pred = []\n",
    "        \n",
    "        for i in range(len(self.learners)):\n",
    "            y_pred.append(self.learners[i].query(X_test))\n",
    "        \n",
    "        return np.mean(y_pred,axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Insane Learner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InsaneLearner(object):\n",
    "    \"\"\"It Implements 20 BagLearners\"\"\"\n",
    "    def __init__(self,verbose=False):\n",
    "        \n",
    "        self.learners =[]\n",
    "        if verbose:\n",
    "            print('InsaneLearner()')\n",
    "            \n",
    "        for i in range(20):\n",
    "            self.learners.append(BagLearner(DTlearner,bags=20,verbose= verbose))\n",
    "            \n",
    "    def add_evidence(self,X_train,y_train):\n",
    "        for i in range(20):\n",
    "            self.learners[i].add_evidence(X_train,y_train)\n",
    "            \n",
    "    def query(self,X_test):\n",
    "        \n",
    "        y_pred  = []\n",
    "        \n",
    "        for i in range(20):\n",
    "            y_pred.append(self.learners[i].query(X_test))\n",
    "            \n",
    "        return np.mean(y_pred,axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Regression Learner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinRegLearner(object):\n",
    "    \n",
    "    def __init__(self):\n",
    "        pass\n",
    "   \n",
    "    # Generating an array of ones greater than the shape of X_train by introducing one extra column\n",
    "    # Replace the X values with X_train\n",
    "    def add_evidence(self,X_train,y_train):\n",
    "        X = np.ones([X_train.shape[0],X_train.shape[1]+1])\n",
    "        X[:,0:X_train.shape[1]] = X_train\n",
    "        \n",
    "        # Calculate the coef of regression\n",
    "        self.coef = np.linalg.lstsq(X, y_train,rcond=None)[0]\n",
    "#         print(self.coef)\n",
    "    # y= mx+c\n",
    "    def query(self,X_test):\n",
    "        return (self.coef[:-1] * X_test).sum(axis = 1) + self.coef[-1]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.01188111,  0.01868924,  0.0221041 ,  0.0067645 ,  0.0221041 ,\n",
       "       -0.03832277,  0.0067645 ,  0.0063445 , -0.02342873, -0.00867184,\n",
       "        0.0067645 , -0.00509224,  0.01281572,  0.00541947,  0.00541947,\n",
       "        0.01516559,  0.01776444,  0.01868924, -0.02342873,  0.0221041 ])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dt = DTlearner(size=1)\n",
    "\n",
    "df = pd.read_csv('Istanbul.csv')\n",
    "df2 = df.drop('date',axis=1)\n",
    "\n",
    "\n",
    "\n",
    "X=df2[:50].drop('EM',axis=1)\n",
    "y=df2[:50]['EM']\n",
    "\n",
    "\n",
    "X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.4)\n",
    "\n",
    "dt.add_evidence(X_train,y_train)\n",
    "dt.query(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.02342873,  0.00609936, -0.02001541, -0.03832277, -0.01477275,\n",
       "       -0.03832277, -0.01909138,  0.00609936,  0.02852446, -0.02001541,\n",
       "       -0.00209765,  0.02852446, -0.02001541, -0.01477275, -0.02342873,\n",
       "       -0.00209765,  0.0221041 ,  0.00541947,  0.00541947, -0.02729444])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rt = RTlearner(size=1)\n",
    "\n",
    "df = pd.read_csv('Istanbul.csv')\n",
    "df2 = df.drop('date',axis=1)\n",
    "\n",
    "\n",
    "\n",
    "X=df2[:50].drop('EM',axis=1)\n",
    "y=df2[:50]['EM']\n",
    "\n",
    "\n",
    "X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.4)\n",
    "\n",
    "rt.add_evidence(X_train,y_train)\n",
    "rt.query(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.01281572, -0.02342873, -0.00555836, -0.00685044,  0.0004087 ,\n",
       "       -0.00482714, -0.02342873,  0.00224254, -0.02001541,  0.00129031,\n",
       "        0.01031968,  0.01091689,  0.01031968,  0.00224254,  0.0063445 ,\n",
       "        0.02490995, -0.01942378, -0.00555836,  0.01868924,  0.00541947])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bl = BagLearner(DTlearner)\n",
    "\n",
    "df = pd.read_csv('Istanbul.csv')\n",
    "df2 = df.drop('date',axis=1)\n",
    "\n",
    "\n",
    "\n",
    "X=df2[:50].drop('EM',axis=1)\n",
    "y=df2[:50]['EM']\n",
    "\n",
    "\n",
    "X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.4)\n",
    "\n",
    "bl.add_evidence(X_train,y_train)\n",
    "bl.query(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.03031151, -0.00334226, -0.00334226,  0.01031968, -0.00334226,\n",
       "        0.00224254, -0.00867665,  0.00877264, -0.00334226,  0.01091689,\n",
       "       -0.00209765, -0.02342873,  0.00967787,  0.01516559, -0.00867665,\n",
       "       -0.00334226, -0.01477275,  0.01091689, -0.02001541,  0.00129031])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Ilearner = InsaneLearner()\n",
    "\n",
    "\n",
    "df = pd.read_csv('Istanbul.csv')\n",
    "df2 = df.drop('date',axis=1)\n",
    "\n",
    "\n",
    "\n",
    "X=df2[:50].drop('EM',axis=1)\n",
    "y=df2[:50]['EM']\n",
    "\n",
    "\n",
    "X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.4)\n",
    "\n",
    "Ilearner.add_evidence(X_train,y_train)\n",
    "Ilearner.query(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20   -0.018936\n",
       "11   -0.023808\n",
       "38    0.010743\n",
       "19   -0.010618\n",
       "28   -0.011267\n",
       "22    0.020625\n",
       "23   -0.002609\n",
       "48   -0.005600\n",
       "35    0.000415\n",
       "18   -0.005243\n",
       "4    -0.002687\n",
       "14   -0.009445\n",
       "46    0.022248\n",
       "7    -0.026924\n",
       "5    -0.029842\n",
       "24    0.022438\n",
       "36   -0.014240\n",
       "17    0.022668\n",
       "49    0.012389\n",
       "45   -0.004135\n",
       "dtype: float64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dt = LinRegLearner()\n",
    "\n",
    "df = pd.read_csv('Istanbul.csv')\n",
    "df2 = df.drop('date',axis=1)\n",
    "\n",
    "\n",
    "\n",
    "X=df2[:50].drop('EM',axis=1)\n",
    "y=df2[:50]['EM']\n",
    "\n",
    "\n",
    "X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.4)\n",
    "\n",
    "dt.add_evidence(X_train,y_train)\n",
    "display(dt.query(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
